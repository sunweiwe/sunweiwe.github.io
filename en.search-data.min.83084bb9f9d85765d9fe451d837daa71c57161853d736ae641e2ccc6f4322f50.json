[{"id":0,"href":"/posts/database/database/syntax/","title":"database syntax","section":"Posts","content":" 数据库 # CRUD # Insert # INSERT INTO user VALUES (10,\u0026#39;root\u0026#39;,\u0026#39;root\u0026#39;,\u0026#34;name@163.com\u0026#34;),(11,\u0026#39;user\u0026#39;,\u0026#39;user\u0026#39;,\u0026#34;name@163.com\u0026#34;); INSERT INTO user(username, password, email) VALUES (\u0026#39;admin\u0026#39;, \u0026#39;admin\u0026#39;, \u0026#39;xxxx@163.com\u0026#39;); Update # UPDATE user SET username=\u0026#39;root\u0026#39;,password=\u0026#39;root\u0026#39; WHERE username = \u0026#39;root\u0026#39;; Delte # DELETE FROM user WHERE username = \u0026#39;root\u0026#39;; // 清空表数据 TRUNCATE TABLE user; Select # SELECT name FROM user; SELECT id,name FROM user; SELECT * FROM user; SELECT DISINCT id FROM user; SELECT * FROM user LIMIT 5; SELECT * FROM user LIMIT 0,5; SELECT * FROM user LIMIT 2,3; 排序 # // desc 降序 SELECT * FROM user ORDER BY create_time ASC, age DESC 分组 # SELECT name, COUNT(address) AS addr_count FROM user GROUP BY name; SELECT name, COUNT(address) AS addr_count FROM user GROUP BY name ORDER BY name DESC; having # 对汇总的 group by 结果进行过滤 一般和 group by 连用 SELECT name, COUNT(*) AS count FROM user WHERE email IS NOT NULL GROUP BY name HAVING COUNT(*) \u0026gt;= 1; 子查询 # where # select column_name [, column_name ] from table1 [, table2 ] where column_name operator (select column_name [, column_name ] from table1 [, table2 ] [where]) from # select column_name [, column_name ] from (select column_name [, column_name ] from table1 [, table2 ] [where]) as temp_table_name where condition IN,BETWEEN # SELECT * FROM user WHERE id IN (\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;); SELECT * FROM user WHERE age BETWEEN 3 AND 5; AND,OR,NOT # SELECT id, name, address FROM user WHERE name = \u0026#39;name\u0026#39; AND age \u0026gt;= 18; SELECT id, name, address FROM user WHERE name = \u0026#39;name\u0026#39; OR age \u0026gt;= 18; SELECT * FROM user WHERE age NOT BETWEEN 8 AND 18; 连接 # 连接表的本质就是将不同表的记录合并起来，形成一张新表。当然，这张新表只是临时的，它仅存在于本次查询期间。\n# join....on select c.cust_name, o.order_num from Customers c inner join Orders o on c.cust_id = o.cust_id order by c.cust_name; # 如果两张表的关联字段名相同，也可以使用USING子句：join....using() select c.cust_name, o.order_num from Customers c inner join Orders o using(cust_id) order by c.cust_name; 连接类型 说明 inner join （默认链接方式）只有两个标都存在满足条件的记录才会返回行。 left join/left outer join 返回左表的所有行，即使右表中没有满足条件的行也是如此。 right join/right outer join 返回右表中的所有行，即使左表没有满足条件的行也是如此。 full join/full outer join 返回其中一个表存在满足条件的记录。 self join 将一个表连接到自身，就像该表是两个表一样。为了区分两个表，在 SQL 语句中需要至少重命名一个表。 cross join 交叉连接，从两个或者多个连接表中返回记录集的笛卡尔积。 组合 # UNION 运算符将两个或更多查询的结果组合起来，并生成一个结果集，其中包含来自 UNION 中参与查询的提取行。\nUNION 基本规则：\n所有查询的列数和列顺序必须相同。 每个查询中涉及表的列的数据类型必须相同或兼容。 通常返回的列名取自第一个查询 SELECT column_name(s) FROM table1 UNION ALL SELECT column_name(s) FROM table2; 函数 # 数据库定义 # 数据库 # CREATE DATABASE test; DROP DATABASE test; USE test; 数据表 # CREATE TABLE user ( id int(10) unsigned NOT NULL COMMENT \u0026#39;Id\u0026#39;, username varchar(64) NOT NULL DEFAULT \u0026#39;default\u0026#39; COMMENT \u0026#39;用户名\u0026#39;, password varchar(64) NOT NULL DEFAULT \u0026#39;default\u0026#39; COMMENT \u0026#39;密码\u0026#39;, email varchar(64) NOT NULL DEFAULT \u0026#39;default\u0026#39; COMMENT \u0026#39;邮箱\u0026#39; ) COMMENT=\u0026#39;用户表\u0026#39;; CREATE TABLE vip_user AS SELECT * FROM user; DROP TABLE user; ALTER TABLE user ADD age int(3); ALTER TABLE user DROP COLUMN age; ALTER TABLE `user` MODIFY COLUMN age tinyint; ALTER TABLE user ADD PRIMARY KEY (id); ALTER TABLE user DROP PRIMARY KEY; 视图 # 定义 # 视图是基于 SQL 语句的结果集的可视化的表。 视图是虚拟的表，本身不包含数据，也就不能对其进行索引操作。对视图的操作和对普通表的操作一样。 作用 # 简化复杂的 SQL 操作，比如复杂的联结； 只使用实际表的一部分数据； 通过只给用户访问视图的权限，保证数据的安全性； 更改数据格式和表示。 CREATE VIEW top_10_user_view AS SELECT id,name FROM user WHERE id \u0026lt; 10; DROP VIEW top_10_user_view; 索引 # 索引是一种用于快速查询和检索数据的数据结构，其本质可以看成是一种排序好的数据结构。\nCREATE INDEX user_index ON user (id); ALTER table user ADD INDEX user_index(id) CREATE UNIQUE INDEX user_index ON user (id); ALTER TABLE user DROP INDEX user_index; 约束 # NOT NULL UNIQUE PRIMARY KEY FOREIGN KEY CHECK DEFAULT CREATE TABLE Users ( Id INT(10) UNSIGNED NOT NULL AUTO_INCREMENT COMMENT \u0026#39;自增Id\u0026#39;, Username VARCHAR(64) NOT NULL UNIQUE DEFAULT \u0026#39;default\u0026#39; COMMENT \u0026#39;用户名\u0026#39;, Password VARCHAR(64) NOT NULL DEFAULT \u0026#39;default\u0026#39; COMMENT \u0026#39;密码\u0026#39;, Email VARCHAR(64) NOT NULL DEFAULT \u0026#39;default\u0026#39; COMMENT \u0026#39;邮箱地址\u0026#39;, Enabled TINYINT(4) DEFAULT NULL COMMENT \u0026#39;是否有效\u0026#39;, PRIMARY KEY (Id) ) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COMMENT=\u0026#39;用户表\u0026#39;; 事务处理 # START TRANSACTION - 指令用于标记事务的起始点。 SAVEPOINT - 指令用于创建保留点。 ROLLBACK TO - 指令用于回滚到指定的保留点；如果没有设置保留点，则回退到 START TRANSACTION 语句处。 COMMIT - 提交事务。 -- 开始事务 START TRANSACTION; -- 插入操作 A INSERT INTO `user` VALUES (1, \u0026#39;root1\u0026#39;, \u0026#39;root1\u0026#39;, \u0026#39;xxxx@163.com\u0026#39;); -- 创建保留点 updateA SAVEPOINT updateA; -- 插入操作 B INSERT INTO `user` VALUES (2, \u0026#39;root2\u0026#39;, \u0026#39;root2\u0026#39;, \u0026#39;xxxx@163.com\u0026#39;); -- 回滚到保留点 updateA ROLLBACK TO updateA; -- 提交事务，只有操作 A 生效 COMMIT; 存储过程 # 代码封装，保证了一定的安全性； 代码复用； 由于是预先编译，因此具有很高的性能。 DROP PROCEDURE IF EXISTS `proc_adder`; DELIMITER ;; CREATE DEFINER=`root`@`localhost` PROCEDURE `proc_adder`(IN a int, IN b int, OUT sum int) BEGIN DECLARE c int; if a is null then set a = 0; end if; if b is null then set b = 0; end if; set sum = a + b; END ;; DELIMITER ; "},{"id":1,"href":"/posts/image/image/basic/","title":"image 基础知识","section":"Posts","content":" image 基础知识\n"},{"id":2,"href":"/posts/linux/linux/cgroup/","title":"cgroup","section":"Posts","content":" for linux cgroup\ncgroup # mount # 目前支持的挂载方式为：\nnsdelegate favordynmods memory_localevents 只能挂载时设置或者通过从 init 命名空间重新挂载来修改，这是系统范围的选项。只用当前 cgroup 的数据填充 memory.events，如果没有这个选项，默认会计数所有子树； memory_recursiveprot 递归地将 memory.min 和 memory.low 保护应用于整个子树，无需显式向下传播到叶节点的 cgroup 中，子树内叶子节点可以自由竞争 组织进程和线程 # 进程 # 每个 cgroup 有一个可读写的的文件 cgroup.procs\n线程 # "},{"id":3,"href":"/posts/linux/linux/ubuntu/","title":"Ubuntu in docker","section":"Posts","content":" 拉取镜像 # docker pull ubuntu 创建容器 # docker run -i -t --name ubuntu ubuntu bash Ubuntu 基本配置 # apt-get update apt-get install vim 更新软件源 # /etc/apt/sources.list\ndeb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse apt-get update apt-get install git 配置 SSH # apt-get install openssh-server 配置 sshd # /etc/ssh/sshd_config\nPermitRootLogin yes # 可以登录 root 用户 PubkeyAuthentication yes # 可以使用 ssh 公钥许可 AuthorizedKeysFile\t.ssh/authorized_keys # 公钥信息保存到文件 .ssh/authorized_keys 中 重启 sshd # /etc/init.d/ssh restart 主机 SSH 公钥 # 在 HOME 目录下创建 .ssh 目录：mkdir ~/.ssh 新建文件 ~/.ssh/authorized_keys ：touch ~/.ssh/authorized_keys 新开一个 macOS 下的终端窗口，执行命令 cat ~/.ssh/id_rsa.pub，复制打印的一行公钥信息 回到 ubuntu 容器中，将第 3 步复制的公钥粘贴到 ~/.ssh/authorized_keys 中保存。 提交修改到镜像 # docker commit -m \u0026lt;Comment\u0026gt; -a \u0026lt;Author\u0026gt; \u0026lt;containerId\u0026gt; \u0026lt;ImageName\u0026gt; "},{"id":4,"href":"/posts/kubernetes/kubernetes/scheduler/","title":"kubernetes scheduler","section":"Posts","content":" kubernetes scheduler\n调度器配置 # 你可以通过编写配置文件，并将其路径传给 kube-scheduler 的命令行参数，定制 kube-scheduler 的行为。\n调度模板（Profile）允许你配置 kube-scheduler 中的不同调度阶段。每个阶段都暴露于某个扩展点中。插件通过实现一个或多个扩展点来提供调度行为。\n你可以通过运行 kube-scheduler \u0026ndash;config 来设置调度模板， 使用 KubeSchedulerConfiguration（v1beta3 或 v1）结构体。\napiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: /etc/srv/kubernetes/kube-scheduler/kubeconfig 说明： 配置文件 # 调度行为发生在一系列阶段中，这些阶段是通过以下扩展点公开的：\n扩展点 # queueSort：这些插件对调度队列中的悬决的 Pod 排序。 一次只能启用一个队列排序插件。 preFilter：这些插件用于在过滤之前预处理或检查 Pod 或集群的信息。 它们可以将 Pod 标记为不可调度。 filter：这些插件相当于调度策略中的断言（Predicates），用于过滤不能运行 Pod 的节点。 过滤器的调用顺序是可配置的。 如果没有一个节点通过所有过滤器的筛选，Pod 将会被标记为不可调度。 postFilter：当无法为 Pod 找到可用节点时，按照这些插件的配置顺序调用他们。 如果任何 postFilter 插件将 Pod 标记为可调度，则不会调用其余插件。 preScore：这是一个信息扩展点，可用于预打分工作。 score：这些插件给通过筛选阶段的节点打分。调度器会选择得分最高的节点。 reserve：这是一个信息扩展点，当资源已经预留给 Pod 时，会通知插件。 这些插件还实现了 Unreserve 接口，在 Reserve 期间或之后出现故障时调用。 permit：这些插件可以阻止或延迟 Pod 绑定。 preBind：这些插件在 Pod 绑定节点之前执行。 bind：这个插件将 Pod 与节点绑定。bind 插件是按顺序调用的，只要有一个插件完成了绑定，其余插件都会跳过。bind 插件至少需要一个。 postBind：这是一个信息扩展点，在 Pod 绑定了节点之后调用。 multiPoint：这是一个仅配置字段，允许同时为所有适用的扩展点启用或禁用插件 调度策略 # 调度器通过 Kubernetes 的监测（Watch）机制来发现集群中新创建且尚未被调度到节点上的 Pod。 调度器会将所发现的每一个未调度的 Pod 调度到一个合适的节点上来运行。 调度器会依据下文的调度原则来做出调度选择。\n调度流程 # 过滤 打分 "},{"id":5,"href":"/posts/docker/docker/create/","title":"从零开始实现一个可用的 docker","section":"Posts","content":" 通过使用 go 实现一个可用的 docker 来了解 docker 的工作原理。\nbasic # 初始化 创建容器 id 下载 image 创建容器工作目录 挂载文件 overlay 方式 创建 veth pair 创建 netns 挂载 veth 支持的命令 # run child-mode setup-netns setup-veth ps exec images rmi container (overlay fs) # 容器的工作目录\n/var/run/container/containers/containerId/fs /var/run/container/containers/containerId/fs/mnt /var/run/container/containers/containerId/fs/upperdir /var/run/container/containers/containerId/fs/workdir mount overlay file system # unix.mount() veth-pair # 虚拟网络设备对,用于解决不同网络命名空间之间的通信\nfunc setupVirtualEthOnHost(containerID string) error { veth0 := \u0026#34;veth0_\u0026#34; + containerID[:6] veth1 := \u0026#34;veth1_\u0026#34; + containerID[:6] linkAttrs := netlink.NewLinkAttrs() linkAttrs.Name = veth0 veth0Struct := \u0026amp;netlink.Veth{ LinkAttrs: linkAttrs, PeerName: veth1, PeerHardwareAddr: createMACAddress(), } if err := netlink.LinkAdd(veth0Struct); err != nil { return err } netlink.LinkSetUp(veth0Struct) containerBridge, _ := netlink.LinkByName(\u0026#34;container0\u0026#34;) netlink.LinkSetMaster(veth0Struct, containerBridge) return nil } "},{"id":6,"href":"/posts/kubernetes/kubernetes/mount/","title":"kubernetes 部署","section":"Posts","content":" 使用 kubeadm 在 linux 上安装 kubernetes\n关闭防火墙 # systemctl disable firewalld systemctl stop firewalld 关闭 selinux # setenforce 0 sed -i \u0026#39;s/SELINUX=permissive/SELINUX=disabled/\u0026#39; /etc/sysconfig/selinux sed -i \u0026#34;s/SELINUX=enforcing/SELINUX=disabled/g\u0026#34; /etc/selinux/config 关闭 swap # swapoff -a sed -i \u0026#39;s/.*swap.*/#\u0026amp;/\u0026#39; /etc/fstab 添加主机名与 IP 对应的关系 # hostnamectl set-hostname k8s-master vim /etc/hosts #添加如下内容： 10.0.24.13 k8s-master 将桥接的 IPV4 流量传递到 iptables 的链 # cat \u0026gt; /etc/sysctl.d/k8s.conf \u0026lt;\u0026lt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system yum # **rm -rfv /etc/yum.repos.d/* curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-8.repo yum install -y yum-utils yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # 配置k8s阿里云源 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF** 基础包 # yum install vim bash-completion net-tools gcc -y docker # yum install docker-ce systemctl start docker # 启动Docker systemctl enable docker # 设置自动启动 docker config # /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://56tyyl4k.mirror.aliyuncs.com\u0026#34; ], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } systemctl daemon-reload systemctl restart docker kubernetes 组件 # yum install -y kubectl kubeadm kubelet systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet 初始化 # kubeadm init --image-repository registry.aliyuncs.com/google_containers --kubernetes-version v1.23.1 --pod-network-cidr=10.0.24.13/16 kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml kubectl config # mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config source \u0026lt;(kubectl completion bash) 污点 # kubectl taint nodes k8s-master node-role.kubernetes.io/master:NoSchedule- "},{"id":7,"href":"/posts/kubernetes/kubernetes/mysql/","title":"stateful 部署 mysql","section":"Posts","content":" kubernetes 使用 stateful 方式部署 mysql\n部署 nfs server # 安装软件包 yum install -y rpcbind nfs-utils 修改 /etc/exports /hone/nfs/ \\*(insecure,rw,sync,no_root_squash) 启动 nfs 服务 mkdir /home/nfs systemctl enable rpcbind systemctl enable nfs-server systemctl start rpcbind systemctl start nfs-server exportfs -r exportfs 创建 RABC # 用户创建 nfs client # apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner namespace: default --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\u0026#39;\u0026#39;] resources: [\u0026#39;persistentvolumes\u0026#39;] verbs: [\u0026#39;get\u0026#39;, \u0026#39;list\u0026#39;, \u0026#39;watch\u0026#39;, \u0026#39;create\u0026#39;, \u0026#39;delete\u0026#39;] - apiGroups: [\u0026#39;\u0026#39;] resources: [\u0026#39;persistentvolumeclaims\u0026#39;] verbs: [\u0026#39;get\u0026#39;, \u0026#39;list\u0026#39;, \u0026#39;watch\u0026#39;, \u0026#39;update\u0026#39;] - apiGroups: [\u0026#39;storage.k8s.io\u0026#39;] resources: [\u0026#39;storageclasses\u0026#39;] verbs: [\u0026#39;get\u0026#39;, \u0026#39;list\u0026#39;, \u0026#39;watch\u0026#39;] - apiGroups: [\u0026#39;\u0026#39;] resources: [\u0026#39;events\u0026#39;] verbs: [\u0026#39;create\u0026#39;, \u0026#39;update\u0026#39;, \u0026#39;patch\u0026#39;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: default roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner namespace: default rules: - apiGroups: [\u0026#39;\u0026#39;] resources: [\u0026#39;endpoints\u0026#39;] verbs: [\u0026#39;get\u0026#39;, \u0026#39;list\u0026#39;, \u0026#39;watch\u0026#39;, \u0026#39;create\u0026#39;, \u0026#39;update\u0026#39;, \u0026#39;patch\u0026#39;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner namespace: default subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: default roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io 部署 nfs client # 用于自动创建 pvc # apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner namespace: default spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: registry.cn-hangzhou.aliyuncs.com/jimywu/nfs-subdir-external-provisioner:v4.0.0 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: k8s-sigs.io/nfs-subdir-external-provisioner - name: NFS_SERVER value: 10.0.24.13 - name: NFS_PATH value: /home/nfs volumes: - name: nfs-client-root nfs: server: 10.0.24.13 path: /home/nfs 创建 StorageClass # 用于 mysql 自动扩容或者所容绑定 pvc # apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: mysql-storage provisioner: k8s-sigs.io/nfs-subdir-external-provisioner reclaimPolicy: Retain volumeBindingMode: Immediate 创建 StatefulSet # apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql-backend spec: selector: matchLabels: app: mysql # 匹配 .spec.template.metadata.labels serviceName: mysql-svc-master replicas: 1 template: metadata: labels: app: mysql # 匹配 .spec.selector.matchLabels spec: initContainers: - name: init-mysql image: mysql:8.0 command: - bash - \u0026#39;-c\u0026#39; - | set ex # Generate mysql server-id from pod ordinal index. [[ `hostname` =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} echo [mysqld] \u0026gt; /mnt/conf.d/server-id.cnf # Add an offset to avoid reserved server-id=0 value. echo server-id=$((100 + $ordinal)) \u0026gt;\u0026gt; /mnt/conf.d/server-id.cnf # Copy appropriate conf.d files from config-map to emptyDir. if [[ $ordinal -eq 0 ]]; then cp /mnt/config-map/master.cnf /mnt/conf.d/ else cp /mnt/config-map/slave.cnf /mnt/conf.d/ fi volumeMounts: - name: conf mountPath: /mnt/conf.d - name: config-map mountPath: /mnt/config-map - name: clone-mysql image: mzmuer/xtrabackup:1.0 command: - bash - \u0026#39;-c\u0026#39; - | set -ex # Skip the clone if data already exists. [[ -d /var/lib/mysql/mysql ]] \u0026amp;\u0026amp; exit 0 # Skip the clone on master (ordinal index 0). [[ `hostname` =~ -([0-9]+)$ ]] || exit 1 ordinal=${BASH_REMATCH[1]} [[ $ordinal -eq 0 ]] \u0026amp;\u0026amp; exit 0 # Clone data from previous peer. ncat --recv-only mysql-ss-$(($ordinal-1)).mysql-svc-master 13306 | xbstream -x -C /var/lib/mysql # Prepare the backup. xtrabackup --prepare --target-dir=/var/lib/mysql volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d containers: - name: mysql image: mysql:8.0 args: [\u0026#39;--default-authentication-plugin=mysql_native_password\u0026#39;] env: - name: MYSQL_ROOT_PASSWORD value: \u0026#39;4rt5$RT%\u0026#39; ports: - name: mysql containerPort: 3306 volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 250m memory: 256Mi limits: cpu: 500m memory: 512Mi livenessProbe: exec: command: [\u0026#39;mysqladmin\u0026#39;, \u0026#39;-uroot\u0026#39;, \u0026#39;-p${MYSQL_ROOT_PASSWORD}\u0026#39;, \u0026#39;ping\u0026#39;] initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 readinessProbe: exec: # Check we can execute queries over TCP (skip-networking is off). command: [\u0026#39;mysqladmin\u0026#39;, \u0026#39;-uroot\u0026#39;, \u0026#39;-p${MYSQL_ROOT_PASSWORD}\u0026#39;, \u0026#39;ping\u0026#39;] initialDelaySeconds: 5 periodSeconds: 2 timeoutSeconds: 1 - name: xtrabackup image: mzmuer/xtrabackup:1.0 ports: - name: xtrabackup containerPort: 13306 command: - bash - \u0026#39;-c\u0026#39; - | set -ex cd /var/lib/mysql # Determine binlog position of cloned data, if any. if [[ -s xtrabackup_slave_info ]]; then # XtraBackup already generated a partial \u0026#34;CHANGE MASTER TO\u0026#34; query # because we\u0026#39;re cloning from an existing slave. mv xtrabackup_slave_info change_master_to.sql.in # Ignore xtrabackup_binlog_info in this case (it\u0026#39;s useless). rm -f xtrabackup_binlog_info elif [[ -f xtrabackup_binlog_info ]]; then # We\u0026#39;re cloning directly from master. Parse binlog position. [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1 rm xtrabackup_binlog_info echo \u0026#34;CHANGE MASTER TO MASTER_LOG_FILE=\u0026#39;${BASH_REMATCH[1]}\u0026#39;,\\ MASTER_LOG_POS=${BASH_REMATCH[2]}\u0026#34; \u0026gt; change_master_to.sql.in fi # Check if we need to complete a clone by starting replication. if [[ -f change_master_to.sql.in ]]; then echo \u0026#34;Waiting for mysqld to be ready (accepting connections)\u0026#34; until mysql -h 127.0.0.1 -e \u0026#34;SELECT 1\u0026#34;; do sleep 1; done echo \u0026#34;Initializing replication from clone position\u0026#34; # In case of container restart, attempt this at-most-once. mv change_master_to.sql.in change_master_to.sql.orig mysql -h 127.0.0.1 \u0026lt;\u0026lt;EOF $(\u0026lt;change_master_to.sql.orig), MASTER_HOST=\u0026#39;mysql-ss-0.mysql-svc-master\u0026#39;, MASTER_USER=\u0026#39;root\u0026#39;, MASTER_PASSWORD=\u0026#39;\u0026#39;, MASTER_CONNECT_RETRY=10; START SLAVE; EOF fi # Start a server to send backups when requested by peers. exec ncat --listen --keep-open --send-only --max-conns=1 13306 -c \\ \u0026#34;xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root\u0026#34; volumeMounts: - name: data mountPath: /var/lib/mysql subPath: mysql - name: conf mountPath: /etc/mysql/conf.d resources: requests: cpu: 100m memory: 100Mi limits: cpu: 200m memory: 200Mi volumes: - name: conf emptyDir: {} - name: config-map configMap: name: mysql volumeClaimTemplates: - metadata: name: data spec: storageClassName: \u0026#39;mysql-storage\u0026#39; accessModes: [\u0026#39;ReadWriteOnce\u0026#39;] resources: requests: storage: 10Gi "},{"id":8,"href":"/posts/docker/docker/basic/","title":"docker basic","section":"Posts","content":" docker 基本的知识\n构建镜像 # docker build -t mysql:v8.0 查看 pod 的日志 # kubectl describe pod podname 进入容器 # kubectl exec -it podname bash 启动容器 # docker run -dit --name emqx -p 18083:18083 -p 1883:1883 -p 8083:8083 -p 8084:8084 -v /etc/localtime:/etc/localtime --restart=always emqx/emqx:latest 查看 container # # 运行的容器 docker ps # 所有的容器 docker ps -a #format docker ps --format \u0026#34;table {{.ID}} {{.Image}} {{.Names}} {{.Ports}}\u0026#34; # 删除停止的容器 docker container prune "},{"id":9,"href":"/posts/go/go/sync/","title":"sync","section":"Posts","content":" go stand library 基本的知识\nMutex # 互斥锁\n互斥锁的实现 # 互斥锁是控制并发的基础手段。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) func main() { var counter Counter var wg sync.WaitGroup wg.Add(10) for i := 0; i \u0026lt; 10; i++ { go func() { defer wg.Done() for j := 0; j \u0026lt; 10000; j++ { counter.Lock() counter.Count++ counter.Unlock() } }() } wg.Wait() fmt.Println(counter.Count) } type Counter struct { sync.Mutex Count uint64 } Pool # 创建池化的对象\nsync.Pool # sync.Pool 数据类型用来保存一组可独立访问的临时对象。\nsync.Pool 本身线程安全，多个 goroutine 可以并发地调用它的方法存取对象。 sync.Pool 不可在使用之后在复制使用。 sync.Pool 的使用方法 # new # pool struct 包含一个 new 字段，这个字段的类型是函数 func() interface{}。当调用 get 方法从池中获取元素的时候，没有空闲的元素可返回时，便会调用 new 新建一个元素返回。\nget # 调用此方法从池中取走一个元素。\nput # 此方法将一个元素返还给 pool，pool 将这个元素保存到池中并且可以复用。\nbuffer 缓冲池 # var buffers ={ New: func() interface{}{ return new(bytes.Buffer) }, } func GetBuffer() *bytes.Buffer{ return buffers.Get().(*bytes.Buffer) } func PutBuffer(buf *bytes.Buffer){ buf.Reset() buffers.Put(buf) } 实现原理 # Get 方法 # func (p *Pool) Get() interface{} { // 把当前 goroutine 固定在当前的p上 l, pid := p.pin() x :=l.private //优先从local 的 private字段取，快速 l.private = nil if x == nil { x,_ := l.shared.popHead() if x == nil{ x = p.getSlow(pid) } } runtime_procUnpin() // 如果没有获取到，尝试使用new函数生成一个新的 if x = nil \u0026amp;\u0026amp; p.New != nil { x = p.New() } return x } 内存泄露 # 使用 sync.Pool 回收 buffer 的时候，一定要检查回收的对象的大小。\n"},{"id":10,"href":"/posts/kubernetes/kubernetes/basic/","title":"kubernetes","section":"Posts","content":" kubernetes 基本的知识\nPod # 指定终止宽限期 # kubectl delete po podname --grace-period=5- token # kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=\u0026#34;{.secrets[0].name}\u0026#34;) -o go-template=\u0026#34;{{.data.token | base64decode}}\u0026#34; docker sercrets # kubectl create secret generic docker \\ --from-file=.dockerconfigjson=/root/.docker/config.json \\ --type=kubernetes.io/dockerconfigjson kubectl # 多集群使用 kubectl # apiVersion: vl clusters: - cluster: certificate-authority: /home/sunweiwe/.minikube/ca.crt server: https://192.168.99.100:8443 name: minikube contexts: - context: cluster: minikube user: minikube namespace: default name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: client-certificate: /home/sunweiwe/ .minikube/apiserver.crt client-key: /home/sunweiwe/ .minikube/apiserver.key kubernetes 源码调试 # "},{"id":11,"href":"/posts/pprof/pprof/basic/","title":"pprof","section":"Posts","content":" pprof 工具可以用来监测进程的运行数据，用于监控程序的性能，对内存使用和 CPU 使用的情况统信息进行分析。\npprof 是什么？ # pprof 工具可以用来监测进程的运行数据，用于监控程序的性能，对内存使用和 CPU 使用的情况统信息进行分析。\n采样方式 # runtime/pprof：采集程序（非 Server）的指定区块的运行数据进行分析。 net/http/pprof：基于 HTTP Server 运行，并且可以采集运行时数据进行分析。 go test：通过运行测试用例，并指定所需标识来进行采集。 使用模式 # Report generation：报告生成。 Interactive terminal use：交互式终端使用。 (go tool pprof http://localhost:6060/debug/pprof/profile/(allocs|blocks|goroutine|etc..)) top list web Web interface：Web 界面。 使用方式 # package main import ( // 略 _ \u0026#34;net/http/pprof\u0026#34; // 会自动注册 handler 到 http server，方便通过 http 接口获取程序运行采样报告 // 略 ) func main() { // 略 runtime.GOMAXPROCS(1) // 限制 CPU 使用数，避免过载 runtime.SetMutexProfileFraction(1) // 开启对锁调用的跟踪 runtime.SetBlockProfileRate(1) // 开启对阻塞操作的跟踪 go func() { // 启动一个 http server，注意 pprof 相关的 handler 已经自动注册过了 if err := http.ListenAndServe(\u0026#34;:6060\u0026#34;, nil); err != nil { log.Fatal(err) } os.Exit(0) }() } 采集数据类型 # allocs 内存分配情况的采样信息 blocks 阻塞操作情况的采样信息 cmdline 显示程序启动命令及参数 goroutine 当前所有协程的堆栈信息 heap 堆上内存使用情况的采样信息 mutex 锁争用情况的采样信息 profile CPU 占用情况的采样信息 threadcreate 系统线程创建情况的采样信息 trace 程序运行跟踪信 "}]